{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbcb3f-ac90-4c52-9fd0-50985d6a9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1942f95-8793-46f1-8932-1daf8155a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster status: 200\n",
      "{\n",
      "  \"name\" : \"elasticsearch-sample-es-default-0\",\n",
      "  \"cluster_name\" : \"elasticsearch-sample\",\n",
      "  \"cluster_uuid\" : \"y5K3JeG9RKy04V1EjNZ2OQ\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.14.0\",\n",
      "    \"build_flavor\" : \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# ----- Elasticsearch config -----\n",
    "ES_URL   = \"https://elasticsearch-sample-es-http.elastic-system.svc:9200\"\n",
    "INDEX    = \"spark-logs-*\"\n",
    "AUTH     = (\"elastic\", \"x8E2PKy5hl9Rx58021oZKJV7\")  # <-- your password\n",
    "\n",
    "# Quick sanity check\n",
    "resp = requests.get(ES_URL, auth=AUTH, verify=False)\n",
    "print(\"Cluster status:\", resp.status_code)\n",
    "print(resp.text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1859bf8b-eadc-4240-9dda-88b890d8a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded first batch: 5000\n",
      "Total loaded so far: 50000\n",
      "Done scrolling. Total docs loaded: 50000\n",
      "Example doc: {'Content': 'Finished task 11.0 in stage 23.0 (TID 471). 2121 bytes result sent to driver', 'EventId': 'E66', 'EventTemplate': 'Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver', 'Occurrences': 2146621}\n"
     ]
    }
   ],
   "source": [
    "TARGET = 50_000        # how many docs you want at minimum\n",
    "BATCH  = 5000          # docs per scroll batch\n",
    "\n",
    "# 1) Initial scroll search\n",
    "init_body = {\n",
    "    \"size\": BATCH,\n",
    "    \"_source\": [\"message\", \"EventId\", \"EventTemplate\", \"Occurrences\"],\n",
    "    \"query\": {\"match_all\": {}},\n",
    "}\n",
    "\n",
    "init = requests.post(\n",
    "    f\"{ES_URL}/{INDEX}/_search?scroll=2m\",\n",
    "    auth=AUTH,\n",
    "    json=init_body,\n",
    "    verify=False,\n",
    ")\n",
    "\n",
    "if init.status_code != 200:\n",
    "    print(init.text)\n",
    "    raise SystemExit(\"Initial scroll failed\")\n",
    "\n",
    "init_json = init.json()\n",
    "scroll_id = init_json.get(\"_scroll_id\")\n",
    "hits = init_json[\"hits\"][\"hits\"]\n",
    "\n",
    "docs = []\n",
    "\n",
    "def add_hits_to_docs(hits, docs_list):\n",
    "    for h in hits:\n",
    "        src = h.get(\"_source\", {})\n",
    "        docs_list.append({\n",
    "            \"Content\": src.get(\"message\"),\n",
    "            \"EventId\": src.get(\"EventId\"),\n",
    "            \"EventTemplate\": src.get(\"EventTemplate\"),\n",
    "            \"Occurrences\": src.get(\"Occurrences\"),\n",
    "        })\n",
    "\n",
    "add_hits_to_docs(hits, docs)\n",
    "print(\"Loaded first batch:\", len(docs))\n",
    "\n",
    "# 2) Continue scrolling\n",
    "while len(docs) < TARGET:\n",
    "    scroll_body = {\n",
    "        \"scroll\": \"2m\",\n",
    "        \"scroll_id\": scroll_id,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(\n",
    "        f\"{ES_URL}/_search/scroll\",\n",
    "        auth=AUTH,\n",
    "        json=scroll_body,\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Scroll error:\", resp.text)\n",
    "        break\n",
    "\n",
    "    data = resp.json()\n",
    "    hits = data[\"hits\"][\"hits\"]\n",
    "\n",
    "    if not hits:\n",
    "        print(\"No more hits, reached end of index.\")\n",
    "        break\n",
    "\n",
    "    add_hits_to_docs(hits, docs)\n",
    "    scroll_id = data.get(\"_scroll_id\", scroll_id)\n",
    "\n",
    "    print(f\"Total loaded so far: {len(docs)}\", end=\"\\r\")\n",
    "\n",
    "print(\"\\nDone scrolling. Total docs loaded:\", len(docs))\n",
    "print(\"Example doc:\", docs[0] if docs else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633e801f-4492-40fd-93dd-31971bd9e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Content: string (nullable = true)\n",
      " |-- EventId: string (nullable = true)\n",
      " |-- EventTemplate: string (nullable = true)\n",
      " |-- Occurrences: long (nullable = true)\n",
      "\n",
      "+----------------------------------------------------------------------------+-------+-------------------------------------------------------------------------+-----------+\n",
      "|Content                                                                     |EventId|EventTemplate                                                            |Occurrences|\n",
      "+----------------------------------------------------------------------------+-------+-------------------------------------------------------------------------+-----------+\n",
      "|Finished task 11.0 in stage 23.0 (TID 471). 2121 bytes result sent to driver|E66    |Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver|2146621    |\n",
      "|Finished task 12.0 in stage 23.0 (TID 472). 2161 bytes result sent to driver|E66    |Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver|2146621    |\n",
      "|Finished task 13.0 in stage 23.0 (TID 473). 2121 bytes result sent to driver|E66    |Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver|2146621    |\n",
      "|Finished task 10.0 in stage 23.0 (TID 470). 2121 bytes result sent to driver|E66    |Finished task <*> in stage <*> (TID <*>). <*> bytes result sent to driver|2146621    |\n",
      "|Got assigned task 475                                                       |E225   |Got assigned task <*>                                                    |2149959    |\n",
      "+----------------------------------------------------------------------------+-------+-------------------------------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in logs_df: 50000\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SparkLogsFromESScroll\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Content\", StringType(), True),\n",
    "    StructField(\"EventId\", StringType(), True),\n",
    "    StructField(\"EventTemplate\", StringType(), True),\n",
    "    StructField(\"Occurrences\", LongType(), True),\n",
    "])\n",
    "\n",
    "rows = []\n",
    "for d in docs:\n",
    "    content   = d.get(\"Content\")\n",
    "    event_id  = d.get(\"EventId\")\n",
    "    template  = d.get(\"EventTemplate\")\n",
    "    occ_raw   = d.get(\"Occurrences\")\n",
    "\n",
    "    try:\n",
    "        occ = int(occ_raw) if occ_raw is not None else None\n",
    "    except (ValueError, TypeError):\n",
    "        occ = None\n",
    "\n",
    "    rows.append((content, event_id, template, occ))\n",
    "\n",
    "logs_df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "logs_df.printSchema()\n",
    "logs_df.show(5, truncate=False)\n",
    "print(\"Total rows in logs_df:\", logs_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0c4bda-5593-4703-b375-22055c1c2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping nulls: 50000\n",
      "Distinct EventIds: 121\n",
      "Sampled rows: 6259\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|Content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |EventId|EventTemplate                                                                                                                                                                                                                                                                                                                                                                                                    |Occurrences|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "|Prepared Local resources Map(__spark__.jar -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.10.34.11\" port: 9000 file: \"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)|E1     |Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)|2561       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows missing Content or EventId\n",
    "logs_small = logs_df.dropna(subset=[\"Content\", \"EventId\"])\n",
    "\n",
    "print(\"Rows after dropping nulls:\", logs_small.count())\n",
    "print(\"Distinct EventIds:\", logs_small.select(\"EventId\").distinct().count())\n",
    "\n",
    "MAX_PER_EVENT = 100  # increase since you have more data now\n",
    "\n",
    "w = Window.partitionBy(\"EventId\").orderBy(F.rand())\n",
    "\n",
    "sampled_df = (\n",
    "    logs_small\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") <= MAX_PER_EVENT)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(\"Sampled rows:\", sampled_df.count())\n",
    "sampled_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23fb9e0-4b4a-419b-8e41-23df324768e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote JSONL to: spark_llm_dataset_50k.jsonl\n",
      "JSONL lines: 6259\n",
      "{\"instruction\": \"Given this Spark log message, identify its template ID and template text.\", \"input\": \"Prepared Local resources Map(__spark__.jar -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\\\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\\\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\\\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)\", \"output\": \"EventId: E1\\nEventTemplate: Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)\"}\n",
      "{\"instruction\": \"Given this Spark log message, identify its template ID and template text.\", \"input\": \"Prepared Local resources Map(__spark__.jar -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/spark-assembly-1.6.0-hadoop2.2.0.jar\\\" } size: 109525492 timestamp: 1485248869215 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/pyspark.zip\\\" } size: 355358 timestamp: 1485248869287 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1485248649253_0001/py4j-0.9-src.zip\\\" } size: 44846 timestamp: 1485248869304 type: FILE visibility: PRIVATE)\", \"output\": \"EventId: E1\\nEventTemplate: Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)\"}\n",
      "{\"instruction\": \"Given this Spark log message, identify its template ID and template text.\", \"input\": \"Prepared Local resources Map(__spark__.jar -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1448006111297_0137/spark-assembly-1.6.0-hadoop2.2.0.jar\\\" } size: 109525492 timestamp: 1459997171809 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1448006111297_0137/pyspark.zip\\\" } size: 355358 timestamp: 1459997172002 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: \\\"hdfs\\\" host: \\\"10.10.34.11\\\" port: 9000 file: \\\"/user/curi/.sparkStaging/application_1448006111297_0137/py4j-0.9-src.zip\\\" } size: 44846 timestamp: 1459997172042 type: FILE visibility: PRIVATE)\", \"output\": \"EventId: E1\\nEventTemplate: Prepared Local resources Map(__spark__.jar -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, pyspark.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>, py4j-<*>-src.zip -> resource { scheme: <*> host: <*> port: <*> file: <*> } size: <*> timestamp: <*> type: <*> visibility: <*>)\"}\n"
     ]
    }
   ],
   "source": [
    "OUT_PATH = \"spark_llm_dataset_50k.jsonl\"\n",
    "\n",
    "sampled_pd = sampled_df.toPandas()\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in sampled_pd.itertuples(index=False):\n",
    "        obj = {\n",
    "            \"instruction\": \"Given this Spark log message, identify its template ID and template text.\",\n",
    "            \"input\": row.Content,\n",
    "            \"output\": f\"EventId: {row.EventId}\\nEventTemplate: {row.EventTemplate}\",\n",
    "        }\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "print(\"Wrote JSONL to:\", OUT_PATH)\n",
    "\n",
    "# sanity: count lines\n",
    "num_lines = sum(1 for _ in open(OUT_PATH, \"r\", encoding=\"utf-8\"))\n",
    "print(\"JSONL lines:\", num_lines)\n",
    "\n",
    "# show first few lines\n",
    "with open(OUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "OUT_PATH = \"spark_llm_dataset_50k.jsonl\"\n",
    "\n",
    "sampled_pd = sampled_df.toPandas()\n",
    "\n",
    "# 1) Write JSONL locally (same as before)\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in sampled_pd.itertuples(index=False):\n",
    "        obj = {\n",
    "            \"instruction\": \"Given this Spark log message, identify its template ID and template text.\",\n",
    "            \"input\": row.Content,\n",
    "            \"output\": f\"EventId: {row.EventId}\\nEventTemplate: {row.EventTemplate}\",\n",
    "        }\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "print(\"Wrote JSONL to:\", OUT_PATH)\n",
    "\n",
    "# 2) Sanity check\n",
    "num_lines = sum(1 for _ in open(OUT_PATH, \"r\", encoding=\"utf-8\"))\n",
    "print(\"JSONL lines:\", num_lines)\n",
    "\n",
    "# 3) Upload to S3\n",
    "s3 = boto3.client(\"s3\")  # uses env vars from the K8s secret\n",
    "\n",
    "BUCKET = \"mas-pipeline-prod\"\n",
    "KEY    = \"spark/spark_llm_dataset_50k.jsonl\"  # path inside the bucket\n",
    "\n",
    "s3.upload_file(OUT_PATH, BUCKET, KEY)\n",
    "\n",
    "print(f\"Uploaded to s3://{BUCKET}/{KEY}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
