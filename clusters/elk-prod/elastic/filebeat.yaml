apiVersion: beat.k8s.elastic.co/v1beta1
kind: Beat
metadata:
  name: filebeat-sample
  namespace: elastic-system
spec:
  type: filebeat
  version: 8.14.0

  daemonSet:
    podTemplate:
      spec:
        containers:
          - name: filebeat
            securityContext:
              runAsUser: 0
            env:
              - name: AWS_ACCESS_KEY_ID
                valueFrom:
                  secretKeyRef:
                    name: filebeat-aws-creds
                    key: AWS_ACCESS_KEY_ID
              - name: AWS_SECRET_ACCESS_KEY
                valueFrom:
                  secretKeyRef:
                    name: filebeat-aws-creds
                    key: AWS_SECRET_ACCESS_KEY
              - name: AWS_REGION
                valueFrom:
                  secretKeyRef:
                    name: filebeat-aws-creds
                    key: AWS_REGION
            volumeMounts: []
        volumes: []

  config:
    filebeat.inputs:
      - type: aws-s3
        id: spark_s3
        enabled: true

        bucket_arn: "arn:aws:s3:::mas-pipeline-prod"
        # If you only care about that one file, use the full key as prefix:
        prefix: "spark/Spark_logs_enriched.csv"

        region_name: "us-west-2"
        interval: 5m

    output.logstash:
      hosts: ["logstash-sample-ls-beats.elastic-system.svc:5044"]
