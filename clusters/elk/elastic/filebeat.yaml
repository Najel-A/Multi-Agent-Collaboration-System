apiVersion: beat.k8s.elastic.co/v1beta1
kind: Beat
metadata:
  name: filebeat-sample
  namespace: elastic-system
spec:
  type: filebeat
  version: 8.14.0
  # You can keep kibanaRef only if you plan to use setup.kibana/dashboard loading.
  # kibanaRef:
  #   name: kibana-sample

  daemonSet:
    podTemplate:
      spec:
        containers:
          - name: filebeat
            securityContext:
              runAsUser: 0
            volumeMounts:
              - name: varlog
                mountPath: /var/log
              - name: varlibdockercontainers
                mountPath: /var/lib/docker/containers
                readOnly: true
              - name: audit-logs-data
                mountPath: /data/audit-logs
                readOnly: true
              - name: spark-logs-data
                mountPath: /data/spark-logs
                readOnly: true
        volumes:
          - name: varlog
            hostPath:
              path: /var/log
          - name: varlibdockercontainers
            hostPath:
              path: /var/lib/docker/containers
          - name: audit-logs-data
            hostPath:
              path: /tmp/audit-logs
              type: DirectoryOrCreate
          - name: spark-logs-data
            hostPath:
              path: /tmp/spark-logs
              type: DirectoryOrCreate

  config:
    filebeat.inputs:
      # Container logs input (keep existing)
      - type: container
        paths:
          - /var/log/containers/*.log
        processors:
          - add_kubernetes_metadata: {}
      
      # CSV audit logs input
      - type: log
        enabled: true
        paths:
          - /data/audit-logs/*.csv
        fields:
          data_source: audit-logs-csv
        fields_under_root: false
        # CSV files are single-line records, Filebeat reads entire line as message
        close_inactive: 5m
        scan_frequency: 10s
      
      # Spark logs input
      - type: log
        enabled: true
        paths:
          - /data/spark-logs/**/*.log
        fields:
          data_source: spark-logs
          log_type: spark_container
        fields_under_root: false
        # Spark logs may have multiline stack traces - match timestamp at start
        multiline.pattern: '^\d{2}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}'
        multiline.negate: false
        multiline.match: after
        close_inactive: 10m
        scan_frequency: 30s

    # Route to Logstash (no ES creds needed)
    output.logstash:
      hosts: ["logstash-sample-ls-beats.elastic-system.svc:5044"]
      # If you enabled TLS on the beats input, add ssl settings here:
      # ssl:
      #   certificate_authorities: ["/mnt/ca/ca.crt"]
